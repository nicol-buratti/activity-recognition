{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import  BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration \n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from activity_dataset import get_dataset_splits\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "\n",
    "\n",
    "MODEL_ID = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
    "OUTPUT_DIR = \"output_llava\"\n",
    "BATCH_SIZE = 1\n",
    "REPO_ID = \"cams01/LLaVa-robot-activity-recognition\"\n",
    "\n",
    "NUM_FRAMES = 100\n",
    "MAX_LENGTH = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a43a01a3313453da3c26bf3cf2a76b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090\n",
      "GPU ID: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    print(f\"GPU ID: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Verifica se PyTorch vede la GPU\n",
    "print(\"Current Device:\", torch.cuda.current_device())  # Mostra l'ID della GPU\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))  # Nome della GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "12.4\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Controlla la versione di PyTorch\n",
    "print(torch.version.cuda)  # Controlla la versione di CUDA\n",
    "print(torch.backends.cudnn.version())  # Controlla la versione di cuDNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e863a1939714c299607ca7057d9d154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the model in half-precision\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(MODEL_ID)\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "processor.image_processor.do_rescale = False\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_dataset_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WindowsPath('atlas_dione_objectdetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection_Study_ActionClips/ATLAS_Dione_ObjectDetection_Study_ActionClips/set06/set06V002.mkv'), 'Suture Pick Up')\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490a654053c64421a9f1b1cc6ac75566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#USE Q_LORA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_decord(video_path, num_frames=NUM_FRAMES):\n",
    "    '''\n",
    "    Decode the video with Decord decoder.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    vr = VideoReader(uri=video_path, ctx=cpu(0)) # you need to install from source to use gpu ctx\n",
    "    indices = np.arange(0, len(vr), len(vr) / num_frames).astype(int)\n",
    "    frames = vr.get_batch(indices).asnumpy()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, label):\n",
    "    # Let's use chat template to format the prompt correctly\n",
    "    video  = read_video_decord(str(video_path))\n",
    "    conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"You are working in an industrial setting where robotic arms perform various activities. Your task is to analyze videos of these robotic arms in action and accurately classify the specific activity being performed in each video. Answer only with the activity detected.\"},\n",
    "                    {\"type\": \"video\"},\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": label},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=False)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=video,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaNextVideoDataCollatorWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # print(\"collator padding:\\t\",{k:v.shape for k, v in features[0].items()})\n",
    "        features = process_video(features[0][0], features[0][1])\n",
    "        # print(\"new features:\\t\", {k:v.shape for k, v in features.items()})\n",
    "        padded_inputs = self.processor.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [feat for feat in features[\"input_ids\"]], # each element is one batch only so we slice [0]\n",
    "                \"attention_mask\": [feat for  feat in features[\"attention_mask\"] ],\n",
    "            },\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # print(\"mid padded:\\t\", {k:v.shape for k, v in padded_inputs.items()})\n",
    "\n",
    "\n",
    "        labels = padded_inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        padded_inputs[\"labels\"] = labels\n",
    "        padded_inputs[\"pixel_values_videos\"] = features['pixel_values_videos'].clone()\n",
    "        \n",
    "        print(\"final padded:\\t\", {k:v.shape for k, v in padded_inputs.items()})\n",
    "\n",
    "        return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    # args related to training\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps=20,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    learning_rate = 2e-05,\n",
    "    max_steps = 10, # adjust this depending on your dataset size\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.1,\n",
    "\n",
    "    # args related to eval/save\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps=20,\n",
    "    save_total_limit = 1,\n",
    "    fp16 = True, # we have the model train and eval with fp16 precision\n",
    "    fp16_full_eval = True,\n",
    "    hub_model_id = REPO_ID,\n",
    "    push_to_hub = True, # wel'll push the model to hub after each epoch\n",
    "\n",
    "\n",
    "    # model that was wrapped for QLORA training with peft will not have arguments listed in its signature\n",
    "    # so we need to pass lable names explicitly to calculate val loss\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    processing_class = processor,\n",
    "    data_collator = LlavaNextVideoDataCollatorWithPadding(processor=processor),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 12:22:03, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14468]), 'attention_mask': torch.Size([1, 14468]), 'labels': torch.Size([1, 14468]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14468]), 'attention_mask': torch.Size([1, 14468]), 'labels': torch.Size([1, 14468]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14468]), 'attention_mask': torch.Size([1, 14468]), 'labels': torch.Size([1, 14468]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14609]), 'attention_mask': torch.Size([1, 14609]), 'labels': torch.Size([1, 14609]), 'pixel_values_videos': torch.Size([1, 101, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14609]), 'attention_mask': torch.Size([1, 14609]), 'labels': torch.Size([1, 14609]), 'pixel_values_videos': torch.Size([1, 101, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14468]), 'attention_mask': torch.Size([1, 14468]), 'labels': torch.Size([1, 14468]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14608]), 'attention_mask': torch.Size([1, 14608]), 'labels': torch.Size([1, 14608]), 'pixel_values_videos': torch.Size([1, 101, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14464]), 'attention_mask': torch.Size([1, 14464]), 'labels': torch.Size([1, 14464]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14465]), 'attention_mask': torch.Size([1, 14465]), 'labels': torch.Size([1, 14465]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 14466]), 'attention_mask': torch.Size([1, 14466]), 'labels': torch.Size([1, 14466]), 'pixel_values_videos': torch.Size([1, 100, 3, 336, 336])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=12.00777587890625, metrics={'train_runtime': 44648.2498, 'train_samples_per_second': 0.002, 'train_steps_per_second': 0.0, 'total_flos': 4.587863974050202e+16, 'train_loss': 12.00777587890625, 'epoch': 1.1176470588235294})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3f86fdb1be40a68e881be48591a9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Informatica_UNICAM\\.cache\\huggingface\\hub\\models--cams01--LLaVa-robot-activity-recognition. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/cams01/LLaVa-robot-activity-recognition/commit/7db8c982e55516c63287c3005e4e6a768b95a7e5', commit_message='Upload model', commit_description='', oid='7db8c982e55516c63287c3005e4e6a768b95a7e5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/cams01/LLaVa-robot-activity-recognition', endpoint='https://huggingface.co', repo_type='model', repo_id='cams01/LLaVa-robot-activity-recognition'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(REPO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

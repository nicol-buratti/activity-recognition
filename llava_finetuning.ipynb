{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import  BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration \n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from activity_dataset import get_dataset_splits\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "\n",
    "\n",
    "MODEL_ID = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
    "OUTPUT_DIR = \"output_llava\"\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "NUM_FRAMES = 200\n",
    "MAX_LENGTH = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090\n",
      "GPU ID: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    print(f\"GPU ID: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Verifica se PyTorch vede la GPU\n",
    "print(\"Current Device:\", torch.cuda.current_device())  # Mostra l'ID della GPU\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))  # Nome della GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "12.4\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Controlla la versione di PyTorch\n",
    "print(torch.version.cuda)  # Controlla la versione di CUDA\n",
    "print(torch.backends.cudnn.version())  # Controlla la versione di cuDNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the model in half-precision\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(MODEL_ID)\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "processor.image_processor.do_rescale = False\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_dataset_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(WindowsPath('atlas_dione_objectdetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection_Study_ActionClips/ATLAS_Dione_ObjectDetection_Study_ActionClips/set11/set11V004.mkv'), 'UVA Tie')\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_decord(video_path, num_frames=NUM_FRAMES):\n",
    "    '''\n",
    "    Decode the video with Decord decoder.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    vr = VideoReader(uri=video_path, ctx=cpu(0)) # you need to install from source to use gpu ctx\n",
    "    indices = np.arange(0, len(vr), len(vr) / num_frames).astype(int)\n",
    "    frames = vr.get_batch(indices).asnumpy()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, label):\n",
    "    # Let's use chat template to format the prompt correctly\n",
    "    video  = read_video_decord(str(video_path))\n",
    "    conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"You are working in an industrial setting where robotic arms perform various activities. Your task is to analyze videos of these robotic arms in action and accurately classify the specific activity being performed in each video. Answer only with the activity detected.\"},\n",
    "                    {\"type\": \"video\"},\n",
    "                    ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": label},\n",
    "                    ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=False)\n",
    "\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=video,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaNextVideoDataCollatorWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # print(\"collator padding:\\t\",{k:v.shape for k, v in features[0].items()})\n",
    "        features = process_video(features[0][0], features[0][1])\n",
    "        print(\"new features:\\t\", {k:v.shape for k, v in features.items()})\n",
    "        padded_inputs = self.processor.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [feat for feat in features[\"input_ids\"]], # each element is one batch only so we slice [0]\n",
    "                \"attention_mask\": [feat for  feat in features[\"attention_mask\"] ],\n",
    "            },\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        print(\"mid padded:\\t\", {k:v.shape for k, v in padded_inputs.items()})\n",
    "\n",
    "\n",
    "        labels = padded_inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        padded_inputs[\"labels\"] = labels\n",
    "        padded_inputs[\"pixel_values_videos\"] = features['pixel_values_videos'].clone()\n",
    "        \n",
    "        print(\"final padded:\\t\", {k:v.shape for k, v in padded_inputs.items()})\n",
    "\n",
    "        return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    # args related to training\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps=20,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    learning_rate = 2e-05,\n",
    "    max_steps = 10, # adjust this depending on your dataset size\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.1,\n",
    "\n",
    "    # args related to eval/save\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps=20,\n",
    "    save_total_limit = 1,\n",
    "    fp16 = True, # we have the model train and eval with fp16 precision\n",
    "    fp16_full_eval = True,\n",
    "\n",
    "    # model that was wrapped for QLORA training with peft will not have arguments listed in its signature\n",
    "    # so we need to pass lable names explicitly to calculate val loss\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    processing_class = processor,\n",
    "    data_collator = LlavaNextVideoDataCollatorWithPadding(processor=processor),\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new features:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'labels': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'labels': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'labels': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'labels': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'labels': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 29009]), 'attention_mask': torch.Size([1, 29009]), 'pixel_values_videos': torch.Size([1, 201, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 29009]), 'attention_mask': torch.Size([1, 29009])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 29009]), 'attention_mask': torch.Size([1, 29009]), 'labels': torch.Size([1, 29009]), 'pixel_values_videos': torch.Size([1, 201, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28864]), 'attention_mask': torch.Size([1, 28864]), 'labels': torch.Size([1, 28864]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28866]), 'attention_mask': torch.Size([1, 28866]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28866]), 'attention_mask': torch.Size([1, 28866])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28866]), 'attention_mask': torch.Size([1, 28866]), 'labels': torch.Size([1, 28866]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "new features:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n",
      "mid padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865])}\n",
      "final padded:\t {'input_ids': torch.Size([1, 28865]), 'attention_mask': torch.Size([1, 28865]), 'labels': torch.Size([1, 28865]), 'pixel_values_videos': torch.Size([1, 200, 3, 336, 336])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Informatica_UNICAM\\Desktop\\csd\\activity-recognition\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

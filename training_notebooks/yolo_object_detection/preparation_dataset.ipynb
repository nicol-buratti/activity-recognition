{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 17974\n",
      "Validation images: 2247\n",
      "Test images: 2246\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "# images_dir = Path(\"atlas_dione_objectdetection/JPEGImages\") # ATLAS dataset directory containing JPG images\n",
    "# annotations_dir = Path(\"atlas_dione_objectdetection/Annotations\") # ATLAS dataset directory containing XML files\n",
    "# test_dir = Path(\"./dataset/test\") # Test directory (only JPG images)\n",
    "\n",
    "images_dir = Path(\"../../atlas_dione_objectdetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection/JPEGImages\") # ATLAS dataset directory containing JPG images\n",
    "annotations_dir = Path(\"../../atlas_dione_objectdetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection/ATLAS_Dione_ObjectDetection/Annotations\") # ATLAS dataset directory containing XML files\n",
    "test_dir = Path(\"./dataset/test\") # Test directory (only JPG images)\n",
    "\n",
    "# Creating directories\n",
    "train_img_dir = Path(\"./dataset/train/images\")\n",
    "train_labels_dir = Path(\"./dataset/train/labels\")\n",
    "val_img_dir = Path(\"./dataset/val/images\")\n",
    "val_labels_dir = Path(\"./dataset/val/labels\")\n",
    "os.makedirs(train_img_dir, exist_ok=True)\n",
    "os.makedirs(val_img_dir, exist_ok=True)\n",
    "os.makedirs(train_labels_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Obtaining list of all images\n",
    "all_images = [f for f in os.listdir(images_dir) if os.path.isfile(os.path.join(images_dir, f))]\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split dataset \n",
    "train_images, val_images, test_images = torch.utils.data.random_split(all_images, [train_ratio, val_ratio, test_ratio])\n",
    "\n",
    "# Moving files in train, val and test directories\n",
    "for img in train_images:\n",
    "    shutil.copy(os.path.join(images_dir, img), os.path.join(train_img_dir, img))\n",
    "\n",
    "for img in val_images:\n",
    "    shutil.copy(os.path.join(images_dir, img), os.path.join(val_img_dir, img))\n",
    "\n",
    "for img in test_images:\n",
    "    shutil.copy(os.path.join(images_dir, img), os.path.join(test_dir, img))\n",
    "\n",
    "print(f\"Training images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(val_images)}\")\n",
    "print(f\"Test images: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertion complete for training!\n",
      "Convertion complete!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "classes = {\"tool\": 0}\n",
    "\n",
    "def convert_bbox(size, box):\n",
    "    # Size is (width, height) of the image\n",
    "    dw = 1.0 / size[0]  # width normalization factor\n",
    "    dh = 1.0 / size[1]  # height normalization factor\n",
    "\n",
    "    xmin, xmax, ymin, ymax = box\n",
    "\n",
    "    # Clamp coordinates to ensure they are within the image bounds\n",
    "    xmin = max(0, xmin)\n",
    "    ymin = max(0, ymin)\n",
    "    xmax = min(size[0], xmax)\n",
    "    ymax = min(size[1], ymax)\n",
    "\n",
    "    # Ensure width and height are positive after clamping\n",
    "    width = max(0, xmax - xmin)\n",
    "    height = max(0, ymax - ymin)\n",
    "\n",
    "    # If width or height is 0, the box is invalid, so return None or an empty tuple\n",
    "    if width == 0 or height == 0:\n",
    "        return None\n",
    "\n",
    "    # Calculate center coordinates\n",
    "    x_center = (xmin + xmax) / 2.0\n",
    "    y_center = (ymin + ymax) / 2.0\n",
    "\n",
    "    # Normalize the center coordinates and dimensions\n",
    "    x_center_norm = x_center * dw\n",
    "    y_center_norm = y_center * dh\n",
    "    width_norm = width * dw\n",
    "    height_norm = height * dh\n",
    "\n",
    "    return (x_center_norm, y_center_norm, width_norm, height_norm)\n",
    "\n",
    "\n",
    "def convert_annotation(xml_file): # Convert a single  XML file in a TXT file in YOLO format\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find(\"size\")\n",
    "    # Height and width are switched in the dataset\n",
    "    height = int(size.find(\"width\").text)\n",
    "    width = int(size.find(\"height\").text)\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        class_name = obj.find(\"name\").text\n",
    "        if class_name not in classes:\n",
    "            continue\n",
    "\n",
    "        bndbox = obj.find(\"bndbox\")\n",
    "\n",
    "        class_id = obj.find(\"pose\").text # Right or left tool\n",
    "        class_id_int = 0\n",
    "        if (class_id == \"Right\"):\n",
    "            class_id_int = 1\n",
    "\n",
    "        xmin = int(bndbox.find(\"xmin\").text)\n",
    "        ymin = int(bndbox.find(\"ymin\").text)\n",
    "        xmax = int(bndbox.find(\"xmax\").text)\n",
    "        ymax = int(bndbox.find(\"ymax\").text)\n",
    "\n",
    "        bbox = convert_bbox((width,height ), (xmin, xmax, ymin, ymax))\n",
    "        if(bbox != None):\n",
    "            output_lines.append(f\"{class_id_int} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "    return output_lines\n",
    "\n",
    "def convertToYOLO(fileToConvert, annotations_dir, labels_dir):\n",
    "    if not fileToConvert.endswith(\".jpg\"):\n",
    "        return\n",
    "\n",
    "    # Corresponding XML files\n",
    "    xml_file = os.path.splitext(fileToConvert)[0] + \".xml\"\n",
    "    input_path = os.path.join(annotations_dir, xml_file)\n",
    "    \n",
    "    # Verifying the XML file exists\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"File XML non trovato per {fileToConvert}, ignorato.\")\n",
    "        return\n",
    "\n",
    "    # Output path for YOLO labels file\n",
    "    labels_path = os.path.join(labels_dir, os.path.splitext(fileToConvert)[0] + \".txt\")\n",
    "   \n",
    "    # Converting XML file in YOLO format\n",
    "    yolo_lines = convert_annotation(input_path)\n",
    "\n",
    "    # Saving TXT file\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        f.writelines(yolo_lines)\n",
    "\n",
    "# Converting all XML files for training\n",
    "for img_train_file in os.listdir(train_img_dir):\n",
    "    convertToYOLO(img_train_file, annotations_dir, train_labels_dir)\n",
    "\n",
    "print(\"Convertion complete for training!\")\n",
    "\n",
    "# Converting all XML files for validation\n",
    "for img_val_file in os.listdir(val_img_dir):\n",
    "    convertToYOLO(img_val_file, annotations_dir, val_labels_dir)\n",
    "    \n",
    "print(\"Convertion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
